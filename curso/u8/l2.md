---
layout: blog
tittle: OpenStack. Redes.
menu:
  - Unidad 8
---

## Nivel de enlace (L2)

Puesto que el despliegue completo incluye muchos elementos y puede resultar
demasiado complejo, vamos a tratar de forma separada el nivel de enlace y el
nivel de red. En esta primera parte, nos centraremos en ver cómo se
interconectan entre sí las máquinas virtuales dentro de una misma red virtual y
cómo se aplican a cada instancia las reglas del grupo de seguridad.

Cuando estamos trabajando en nivel de enlace, los parámetros significativos en
este nivel son los switches virtuales o bridges, puertos de los mismos y las
direcciones MAC de las interfaces de red conectadas.

## Bridge de integración y túneles GRE

El objetivo a la hora de utilizar virtualización de redes es conseguir que las
instancias de una misma red se comuniquen entre sí utilizando el mismo
direccionamiento y compartiendo algunos elementos comunies (servidor DHCP,
puerta de enlace, etc.), pero que permenezcan completamente separadas del resto
de redes virtuales. Esto debe producirse independientemente del hipervisor (nodo
de computación) en el que se estén ejecutando las máquinas virtuales y una de
las formas de conseguir esto con OpenvSwitch es crear un switch virtual en cada
nodo, que se denomina switch de integración o br-int y comunicar cada nodo con
los demás a través de túneles GRE, que garantizan la seguridad de las
comunicaciones entre nodos.

Un esquema que representa esta situación tras una instalación limpia de
OpenStack neutron en cualquiera de los escenarios de prueba se ve en la
siguiente imagen:

<div style="text-align: center;"><img src="img/red-limpia.png" alt="red limpia"/></div>

### Verificación inicial

Puesto que vamos a ir mostrando paso a paso cómo se van creando algunos bridges
y puertos de conexión, vamos a mostrar cual es la situación de partida.

Inicialmente no hay ningún bridge linux:

	# brctl show
	bridge name	bridge id		STP enabled	interfaces

En el nodo de red hay tres puentes Open vSwitch (br-int, br-tun y br-ex). En
este apartado trabajaremos con el bridge de integración (br-int) y el bridge
para los túneles (br-tun):

	# ovs-vsctl list-br
	br-ex
	br-int
	br-tun

Esa misma salida en un nodo de computación, muestra que hay dos bridges Open
vSwitch:

	# ovs-vsctl list-br
	br-int
	br-tun

El único dispositivo conectado inicialmente a br-int es br-tun a través de una
interfaz especial denominada puerto patch:

	# ovs-ofctl show br-int
	OFPT_FEATURES_REPLY (xid=0x2): dpid:000032e2b607a841
	n_tables:254, n_buffers:256
	capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP
	actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_D...
	 1(patch-tun): addr:fa:82:c1:68:15:38
	     config:     0
	     state:      0
	     speed: 0 Mbps now, 0 Mbps max
	 LOCAL(br-int): addr:32:e2:b6:07:a8:41
	     config:     0
	     state:      0
	     speed: 0 Mbps now, 0 Mbps max
	OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0

### Creación de la primera red y subred

Utilizando el cliente de línea de comandos de neutron, creamos una red privada y
una subred asociada a ella con el rango de direcciones IP 10.0.0.0/24:

	$ neutron net-create privada
	Created a new network:
	+----------------+--------------------------------------+
	| Field          | Value                                |
	+----------------+--------------------------------------+
	| admin_state_up | True                                 |
	| id             | f84a328e-407e-4ca6-87bb-ec148853c585 |
	| name           | privada                              |
	| shared         | False                                |
	| status         | ACTIVE                               |
	| subnets        |                                      |
	| tenant_id      | 4beb810ce40f49659e0bca732e4f1a3c     |
	+----------------+--------------------------------------+

	$ neutron subnet-create f84a328e-407e-4ca6-87bb-ec148853c585 10.0.0.0/24
	Created a new subnet:
	+------------------+--------------------------------------------+
	| Field            | Value                                      |
	+------------------+--------------------------------------------+
	| allocation_pools | {"start": "10.0.0.2", "end": "10.0.0.254"} |
	| cidr             | 10.0.0.0/24                                |
	| dns_nameservers  |                                            |
	| enable_dhcp      | True                                       |
	| gateway_ip       | 10.0.0.1                                   |
	| host_routes      |                                            |
	| id               | d4bb2d0e-2af7-44fe-9729-4e3b95766e28       |
	| ip_version       | 4                                          |
	| name             |                                            |
	| network_id       | f84a328e-407e-4ca6-87bb-ec148853c585       |
	| tenant_id        | 4beb810ce40f49659e0bca732e4f1a3c           |
	+------------------+--------------------------------------------+

### Creación automática de la red virtual al levantar una instancia:

Al lanzar una instancia, se crean todos los dispositivos de red necesarios y se
interconectan adecuadamente:

	$ nova boot --image 4ddb27ab-b3cc-4a65-ac52-f4ce7894e4ed \
	--flavor 1 \
	--key_name clave-openstack \
	--nic net-id=f84a328e-407e-4ca6-87bb-ec148853c585 \
	test

Comprobamos que aparecen un nuevo puerto en el br-int del nodo de red:

	# ovs-ofctl show br-int
	...
	 2(tap79712efb-32): addr:30:02:00:00:00:00
	     config:     PORT_DOWN
	     state:      LINK_DOWN
	     speed: 0 Mbps now, 0 Mbps max
	...

y un nuevo puerto en el nodo de computación:

	# ovs-ofctl show br-int
	...
	 2(qvob079c24c-83): addr:ae:49:25:f2:4a:1c
	     config:     0
	     state:      0
	     current:    10GB-FD COPPER
	     speed: 10000 Mbps now, 0 Mbps max
	...

Y vemos que se etiqueta automáticamente ambos nodos en una misma VLAN con el
número 1, tanto en el nodo de red:

	# ovs-vsctl show
	...
	        Port "tap79712efb-32"
	            tag: 1
	            Interface "tap79712efb-32"
	                type: internal
	...

Como en el nodo de computación:

	# ovs-vsctl show
	...
	    Bridge br-int
	        Port "qvob079c24c-83"
	            tag: 1
	            Interface "qvob079c24c-83"
	...

Para entender la numeración que utiliza neutron para los nuevos puertos y los
pasos que se siguen, vamos a ver en primer lugar los nuevos puertos que aparecen
en el proyecto (estos puertos son los que el usuario del proyecto puede ver):

	$ neutron port-list
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
	| id                                   | name | mac_address       | fixed_ips                                                                       |
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
	| 79712efb-320d-407d-82d7-eafd7eb51abe |      | fa:16:3e:dd:ff:a4 | {"subnet_id": "d4bb2d0e-2af7-44fe-9729-4e3b95766e28", "ip_address": "10.0.0.3"} |
	| b079c24c-8386-47db-a730-35b3a99419e8 |      | fa:16:3e:79:3b:ac | {"subnet_id": "d4bb2d0e-2af7-44fe-9729-4e3b95766e28", "ip_address": "10.0.0.2"} |
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+

El que tiene la dirección IP 10.0.0.3 se corresponde con un servidor DHCP que se ha 
creado (que veremos más adelante), el que tiene la dirección IP 10.0.0.2 se
corresponde con la máquina que hemos creado, por lo que nos quedamos con su uuid:

	 b079c24c-8386-47db-a730-35b3a99419e8

Como se tienen que crear varias interfaces de red asociadas a este puerto y el UUID es 
demasiado largo, neutron se queda con los 11 primeros caracteres (b079c24c-83).

Podemos comprobar que en el nodo de computación en el que se está ejecutando la
instancia aparecen 4 interfaces de red nuevas relacionadas con el identificador
b079c24c-83:

	13: qbrb079c24c-83: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN 
	    link/ether de:a1:64:29:9d:40 brd ff:ff:ff:ff:ff:ff
	14: qvob079c24c-83: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
	    link/ether ae:49:25:f2:4a:1c brd ff:ff:ff:ff:ff:ff
	15: qvbb079c24c-83: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
	    link/ether de:a1:64:29:9d:40 brd ff:ff:ff:ff:ff:ff
	16: tapb079c24c-83: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 500
	    link/ether fe:16:3e:79:3b:ac brd ff:ff:ff:ff:ff:ff

La forma en la que se interconectan todas estas interfaces se muestra en la siguiente
 imagen y es necesario añadir un bridge linux por cada instancia por la forma de definir
las reglas de seguridad por instancia:

<div style="text-align: center;"><img src="img/mv1.png" alt="mv1"/></div>

	qbr: Linux bridge
	qvo: Puerto en br-int
	qvb: Puerto en linux bridge
	tap: Interfaz de red de la máquina

Esto es algo que choca bastante al principio y hace más difícil entender los
elementos implicados, pero parece ser que existe una incompatibilidad
actualmente entre la utilización de Open vSwitch, iptables e interfaces tap, tal
como se explica en <a
href="http://docs.openstack.org/admin-guide-cloud/content/under_the_hood_openvswitch.html">OpenStack
Cloud Administration Guide</a>: *Ideally, the TAP device vnet0 would be
connected directly to the integration bridge, br-int. Unfortunately, this isn't
possible because of how OpenStack security groups are currently
implemented. OpenStack uses iptables rules on the TAP devices such as vnet0 to
implement security groups, and Open vSwitch is not compatible with iptables
rules that are applied directly on TAP devices that are connected to an Open
vSwitch port.*

Comprobamos el nuevo bridge linux que se ha creado con las interfaces tapb079c24c-83 
y qvbb079c24c-83

	# brctl show qbrb079c24c-83
	bridge name	bridge id		STP enabled	interfaces
	qbrb079c24c-83		8000.dea164299d40	no		qvbb079c24c-83
									tapb079c24c-83

### Servidor DHCP

Se utiliza linux network namespaces, un modo de virtualización de la red

En el nodo de red ejecutamos la instrucción:

	# ip netns
	qdhcp-f84a328e-407e-4ca6-87bb-ec148853c585

Vemos que se ha creado un espacio de nombres que empieza por dhcp, podemos ejecutar 
comandos en ese espacio de nombres de forma independiente del sistema:

	[root@rdo log]# ip netns exec qdhcp-f84a328e-407e-4ca6-87bb-ec148853c585 ip link show
	11: tap79712efb-32: <BROADCAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN 
	    link/ether fa:16:3e:dd:ff:a4 brd ff:ff:ff:ff:ff:ff
	12: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN 
	    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

Comprobamos que el dispositivo tap79712efb-32 está definido en este espacio de nombres.

Podemos ver también el proceso DHCP (con dnsmasq) que se está ejecutando en el
nodo de red  para darle IP dinámica a las instancias:

	# ps aux|grep f84a328e-407e-4ca6-87bb-ec148853c585
	nobody   19821  0.0  0.0  12888   652 ?        S    10:15   0:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tap79712efb-32 --except-interface=lo --pid-file=/var/lib/neutron/dhcp/f84a328e-407e-4ca6-87bb-ec148853c585/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/f84a328e-407e-4ca6-87bb-ec148853c585/host --dhcp-optsfile=/var/lib/neutron/dhcp/f84a328e-407e-4ca6-87bb-ec148853c585/opts --leasefile-ro --dhcp-range=tag0,10.0.0.0,static,86400s --dhcp-lease-max=256 --conf-file= --domain=openstacklocal

Podemos acceder al directorio:

	# cd /var/lib/neutron/dhcp/f84a328e-407e-4ca6-87bb-ec148853c585

Y comprobar que existe una reserva para la MAC de la instancia:

	# cat /var/lib/neutron/dhcp/f84a328e-407e-4ca6-87bb-ec148853c585/host 
	fa:16:3e:79:3b:ac,host-10-0-0-2.openstacklocal,10.0.0.2

### Conexión con otra instancia de la misma red

Creamos una nueva instancia en la misma red, listamos los puertos que aparecen
en el proyecto y comprobamos que aparece un puerto nuevo:

	$ neutron port-list
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
	| id                                   | name | mac_address       | fixed_ips                                                                       |
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
	| 6a6ac39c-61fb-46a5-bd39-e8ba1701d103 |      | fa:16:3e:eb:2c:d3 | {"subnet_id": "d4bb2d0e-2af7-44fe-9729-4e3b95766e28", "ip_address": "10.0.0.4"} |
	...

Luego tiene que haber nuevas interfaces de red:

	# ip link show|grep 6a6ac39c-61
	17: qbr6a6ac39c-61: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN 
	18: qvo6a6ac39c-61: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
	19: qvb6a6ac39c-61: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
	20: tap6a6ac39c-61: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 500

Siguiendo el esquema de trabajo de la primera imagen, esta instancia se ha
creado en el segundo nodo de computación, por lo que comprobamos que
qvo6a6ac39c-61 está conectado a br-int:

	# ovs-ofctl show br-int
	...
	 2(qvo6a6ac39c-61): addr:0e:7e:7b:30:4a:bc
	     config:     0
	     state:      0
	     current:    10GB-FD COPPER
	     speed: 10000 Mbps now, 0 Mbps max
	...

Y vemos el nuevo bridge linux:

	# brctl show qbr6a6ac39c-61
	bridge name	bridge id		STP enabled	interfaces
	qbr6a6ac39c-61		8000.96a4ec71dea6	no		qvb6a6ac39c-61
									tap6a6ac39c-61

### Redes independientes

Utilizamos otro proyecto y creamos una red y una subred con el mismo
direccionamiento 10.0.0.0/24

Al lanzar la primera instancia, comprobamos los puertos que se han creado:

	$ neutron port-list
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
	| id                                   | name | mac_address       | fixed_ips                                                                       |
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
	| 03b89c06-87a6-4532-ac7c-397e0b537bfa |      | fa:16:3e:f8:83:5a | {"subnet_id": "16786d0d-2252-4b86-9f5b-e1e5741b0a14", "ip_address": "10.0.0.2"} |
	| 319ca3b4-5fe7-4f45-9b8a-eef0dfda0d87 |      | fa:16:3e:52:c2:89 | {"subnet_id": "16786d0d-2252-4b86-9f5b-e1e5741b0a14", "ip_address": "10.0.0.3"} |
	+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+

De nuevo aparecen dos nuevos puertos y lógicamente se ha creado toda la
estructura similar a la de antes, en particular un nuevo proceso DHCP en el nodo
de red:

	# ip netns
	qdhcp-f84a328e-407e-4ca6-87bb-ec148853c585
	qdhcp-60346ea0-ed9d-40aa-a615-5dd3e33892bc

	# ps aux |grep --color 60346ea0-ed9d-40aa-a615-5dd3e33892bc
	nobody    1142  0.0  0.0  12888   640 ?        S    13:26   0:00 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tap319ca3b4-5f --except-interface=lo --pid-file=/var/lib/neutron/dhcp/60346ea0-ed9d-40aa-a615-5dd3e33892bc/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/60346ea0-ed9d-40aa-a615-5dd3e33892bc/host --dhcp-optsfile=/var/lib/neutron/dhcp/60346ea0-ed9d-40aa-a615-5dd3e33892bc/opts --leasefile-ro --dhcp-range=tag0,10.0.0.0,static,86400s --dhcp-lease-max=256 --conf-file= --domain=openstacklocal

Vemos que se ha etiquetado una nueva VLAN con el número 2:

	# ovs-vsctl show
	...
	        Port "qvo03b89c06-87"
	            tag: 2
	            Interface "qvo03b89c06-87"
	...

En la que están conectados el servidor DHCP y el nuevo bridge linux.

Un esquema que representa la creación de las tres instancias es el siguiente:

<div style="text-align: center;"><img src="img/red-2-tags.png" alt="red 2 tags"/></div>

### Referencias

* <a
  href="http://docs.openstack.org/grizzly/openstack-network/admin/content/ch_preface.html">OpenStack
  Networking Administration Guide</a>
* <a href="http://openstack.redhat.com/Networking_in_too_much_detail">RDO -
  Networking in too much detail </a>

